---
title: "API Examples"
output:
  html_document:
    theme: cosmo
    highlight: zenburn
    css: "note-style.css"
---

```{r, include=FALSE, message=FALSE}
source("start.R")
source("cache.R")

food <- read_csv("data/food.csv")
```

## Example Cache

Let's start with a relatively simple API that tells the current time in
different time zones. This is probably not a good candidate for caching, but
let's use it anyway as an example. To start, we specify the URL using the
protocol, authority, path, and query parameters.

```{r}
url_str <- modify_url(
  "https://www.timeapi.io/api/Time/current/zone",
  query = list("timeZone" = "Europe/Amsterdam")
)
```

Next, we call the HTTP GET method using our cache wrapper function. The result
can be parsed as JSON using the `content` function and and appropriate type.

```{r}
res <- http_cache_get(url_str, cache_dir = "cache")
obj <- content(res, type = "application/json")
```

The object `obj` looks like the lists that we saw last time from parsed JSON
data. Let's grab a few of the parts:

```{r}
df <- tibble(
  timezone = obj$timeZone,
  datetime = obj$dateTime,
  dow = obj$dayOfWeek
)
df
```

If we wanted clear the cache of page, we can use this helper function:

```{r, eval=FALSE}
http_cache_clear(cache_dir = "cache")
```

## Wikipedia Page Views

Next, let's grab the Wikipedia page views data that we worked with last time,
but now using the API. Again, we will start with defining a base URL using the
protocol, authority, path, and query parameters.

```{r}
url_base <- modify_url(
  "https://en.wikipedia.org/w/api.php",
  query = list(action = "query", format = "json", prop = "pageviews")
)
```

To fetch a particular page, all we need to do is add an additional parameter
called titles that contains the page title you want to grab.

```{r}
url_str <- modify_url(url_base, query = list(titles = "apple"))
res <- http_cache_get(url_str, cache_dir = "cache")
obj <- content(res, type = "application/json")
```

And as before, we can turn this into a rectangular data set by parsing the
list `obj`.

```{r}
dt <- tibble(
  page = obj$query$pages[[1]]$title,
  date = ymd(names(obj$query$pages[[1]]$pageviews)),
  views = flatten_int(obj$query$pages[[1]]$pageviews)
)
dt
```

Now, let's try to cycle through all of the pages in the `food` dataset. 

```{r}
n <- nrow(food)
output <- vector("list", length = n)

for (i in seq_len(n))
{
  url_str <- modify_url(url_base, query = list(titles = food$wiki[i]))
  res <- http_cache_get(url_str, cache_dir = "cache")
  
  stop_for_status(res)

  obj <- content(res, type = "application/json")
  output[[i]] <- tibble(
    page = obj$query$pages[[1]]$title,
    date = ymd(names(obj$query$pages[[1]]$pageviews)),
    views = flatten_int(obj$query$pages[[1]]$pageviews),
    wiki = food$wiki[i]
  )
}

output <- bind_rows(output)
output
```

And now we can work directly with the data using the visualization and data
manipulation techniques we have learned in the previous weeks. 

```{r, fig.height = 4}
output %>%
  group_by(page, wiki) %>%
  summarize(total_views = sum(views)) %>%
  inner_join(food, by = "wiki") %>%
  ungroup() %>%
  arrange(desc(total_views)) %>%
  mutate(page = fct_inorder(page)) %>%
  mutate(food_group = stri_trans_totitle(food_group)) %>%
  ggplot(aes(total_views, page)) +
    geom_col(aes(fill = food_group), show.legend = FALSE) +
    scale_fill_viridis_d() +
    facet_wrap(~food_group, scales = "free") +
    labs(x = "", y = "Page Views")
```

## Continuation over an API

As a final example here, let's see another common API pattern using an API 
provided by the Library of Congress. We start by creating a base url:

```{r}
url_base <- modify_url(
  "https://www.loc.gov/collections/fsa-owi-color-photographs",
  query = list(fo = "json", c = "150")
)
```

Making a request on the base API returns the first 150 results. 

```{r}
res <- http_cache_get(url_base, cache_dir = "cache")
obj <- content(res, type = "application/json")
```

Which we can parse using some map functions:

```{r}
df <- tibble(
  contributor =  map_chr(obj$results, ~ .x$contributor[[1]] ),
  title = map_chr(obj$results, ~ .x$title),
  year = map_chr(obj$results, ~ .x$date),
  state = map_chr(obj$results, ~ null_to_na(.x$location_state[[1]])),
  city = map_chr(obj$results, ~ null_to_na(.x$location_city[[1]]))
)
df
```

The results contain another field called pagination, which has a key called
"next" (a reserved word in R, so you need the quotes). It tells us the URL
needed to grab the next 150 results:

```{r}
obj$pagination$"next"
```

And, we can directly call this URL to get the next page:

```{r}
url_next <- obj$pagination$"next"
res <- http_cache_get(url_next, cache_dir = "cache")
obj <- content(res, type = "application/json")
```

Which in turn has another next field with the next set of results:

```{r}
obj$pagination$"next"
```

In this example, we can easily see the pattern that the query parameter "sp" 
contains the page number. However, often this is not the case you need to use 
the next URL field, so let's do that here.

We want to cycle through the pages until the next field is missing, which 
requires a little bit more programming:

```{r}
# create an empty list to store the output in
output <- list()

# set the starting url
url_str <- url_base

# loop through the data until the "next" string is NULL (that means we have hit
# the end of the data)
while (!is.null(url_str))
{
  # grab the next page of data
  res <- http_cache_get(url_str, cache_dir = "cache")
  stop_for_status(res)
  obj <- content(res, type = "application/json")

  # parse the data
  df <- tibble(
    contributor = map_chr(obj$results, ~ null_to_na(.x$contributor[[1]])),
    title = map_chr(obj$results, ~ .x$title),
    year = map_chr(obj$results, ~ null_to_na(.x$date)),
    state = map_chr(obj$results, ~ null_to_na(.x$location_state[[1]])),
    city = map_chr(obj$results, ~ null_to_na(.x$location_city[[1]]))
  )

  # save the results by appending to the output list
  output <- c(output, list(df))
  
  # set the next page's URL; will be null at the end
  url_str <- obj$pagination$"next"
}

output <- bind_rows(output)
output
```

And just for fun, let's create a plot of the output:

```{r}
output %>%
  group_by(contributor, year) %>%
  summarize(n = n()) %>%
  ungroup() %>%
  arrange(desc(n)) %>%
  filter(!is.na(contributor)) %>%
  mutate(contributor = stri_trans_totitle(contributor)) %>%
  mutate(contributor = fct_inorder(contributor)) %>%
  ggplot(aes(n, contributor)) +
    geom_col(aes(fill = year)) +
    labs(x = "Number of Photographs", y = "Photographer", fill = "Year")
```







